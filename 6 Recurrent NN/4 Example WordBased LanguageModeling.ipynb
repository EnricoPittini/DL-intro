{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uxck908BGL3f"
   },
   "source": [
    "# Example: Word-based Language Modeling\n",
    "\n",
    "In the previous notebook we have seen an example of a character-based language model. Let's improve that, by building a word-based language model.\n",
    "\n",
    "We still build our language model based on a sequence of $N=10$ words.\n",
    "\n",
    "In addition, this time we use LSTM instead of SimpleRNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6eC9TPW1RZOu"
   },
   "outputs": [],
   "source": [
    "N=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQmPeUTR2Tzl"
   },
   "source": [
    "## DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-61eOpTmtiIR"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8YX5TWJ2-vA"
   },
   "source": [
    "### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5l5Hykd8Q5jx"
   },
   "outputs": [],
   "source": [
    "INPUT_FILE = \"wonderland.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gATu48qfRB_8"
   },
   "outputs": [],
   "source": [
    "text_file = open(INPUT_FILE, 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "SvgFAu_CQ5nv"
   },
   "outputs": [],
   "source": [
    "# List, which will contain all the lines of the book. Each line is stored as a string\n",
    "lines = []\n",
    "for line in text_file:\n",
    "    line = line.strip()\n",
    "    # We transform all the characters to lowercase\n",
    "    line = line.lower()\n",
    "    line = line.decode(\"ascii\", \"ignore\")\n",
    "    #line = line.\n",
    "    # We skip empty lines\n",
    "    if len(line) == 0:\n",
    "        continue\n",
    "    lines.append(line)\n",
    "#text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "cIgTOfUqUfsW"
   },
   "outputs": [],
   "source": [
    "# Single string, containing the whole text\n",
    "text = \" \".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nPiCEg9oVnI1"
   },
   "outputs": [],
   "source": [
    "# List of words\n",
    "words_text = text.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qz54-DAwVtfO",
    "outputId": "fa4079f7-e0c7-42ec-a300-bf0cb5142de7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['project',\n",
       " 'gutenbergs',\n",
       " 'alices',\n",
       " 'adventures',\n",
       " 'in',\n",
       " 'wonderland,',\n",
       " 'by',\n",
       " 'lewis',\n",
       " 'carroll',\n",
       " 'this']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X-qXzMyGV_3N",
    "outputId": "45159c12-40b9-4d91-9b66-1439bf486fdf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29697"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_rMpa833CSA"
   },
   "source": [
    "### Set of all possible words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6y_lzTvSQ5sF",
    "outputId": "a408d1e9-6fba-48ab-9d64-07872a9e880c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5071"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set of all the possible words in our dataset\n",
    "words = set(words_text)\n",
    "\n",
    "# Number of all the possible words\n",
    "n_words = len(words)\n",
    "n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUFx7Dbv3Q44"
   },
   "source": [
    "### PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92VxY4bH3Geg"
   },
   "source": [
    "### Mapping words into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "4aGDSr2nQ5xF"
   },
   "outputs": [],
   "source": [
    "# Dictionary, which maps words into the corresponding integers (i.e. indeces) \n",
    "word2index = dict((w, i) for i, w in enumerate(words))\n",
    "\n",
    "# Dictionary, which maps integers/indeces into the corresponding words \n",
    "index2word = dict((i, w) for i, w in enumerate(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Apg_o5EI3LOQ"
   },
   "source": [
    "### Inputs and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YYpfKx8NRRw0"
   },
   "outputs": [],
   "source": [
    "# List which will contain all the possible instances x, which are all the possible sequences of N adjacent words\n",
    "inputs = []\n",
    "\n",
    "# List which will contain the targets for the corresponding instances x\n",
    "targets = []\n",
    "\n",
    "# We iterate over all the possible words in the text\n",
    "# Actually, we don't consider the last N characters\n",
    "for i in range(0, len(words_text)-N):\n",
    "  # Instance x: it consists in the N consecutive words starting from the index 'i'\n",
    "  x = words_text[i : i+N]\n",
    "  inputs.append(x)\n",
    "\n",
    "  # Target corresponding to the instance x: it is the word after N characters. It is the word rigth after the sequence\n",
    "  y = words_text[i+N]\n",
    "  targets.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GlxlIlbHWVqV",
    "outputId": "aa9efde9-870b-4688-d877-e3ca4f643717"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['project',\n",
       " 'gutenbergs',\n",
       " 'alices',\n",
       " 'adventures',\n",
       " 'in',\n",
       " 'wonderland,',\n",
       " 'by',\n",
       " 'lewis',\n",
       " 'carroll',\n",
       " 'this']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "QEt-ue7KWX39",
    "outputId": "ad80e59e-9748-4c81-fea5-4b61ab44b9c4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'ebook'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wVxUpA5JRVHc",
    "outputId": "d98c1114-372e-4a28-e886-aa88c718aac0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29687"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of instances x in our dataset, where an instance x is a sequence of N=10 consecutive words\n",
    "M = len(inputs)\n",
    "M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4nN5ENQ3VOQ"
   },
   "source": [
    "### Transforming words into integers\n",
    "We transform each word into the corresponding integer/index. We do that both in the inputs and in the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AMwI9cl4WhJU",
    "outputId": "088ab650-684d-4563-ac1a-9a68c631b9b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[601, 2294, 1035, 4302, 1009, 1986, 3420, 125, 4875, 920]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_integers = [[word2index[w] for w in input] for input in inputs]\n",
    "inputs_integers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RDSNRUJWWy1N",
    "outputId": "66af1fb5-3d82-4eda-abb2-d0dfb712fdae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_integers = [word2index[target] for target in targets]\n",
    "targets_integers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3JhMvQi3knk"
   },
   "source": [
    "### Next preprocessing step\n",
    "The next preprocessing step could be to one-hot encode the words, as seen in the last notebook for characters.\n",
    "\n",
    "However, we follow a better approach: word embeddings.\n",
    "\n",
    "https://machinelearningmastery.com/what-are-word-embeddings/#:~:text=A%20word%20embedding%20is%20a,challenging%20natural%20language%20processing%20problems.\n",
    "\n",
    "https://www.tensorflow.org/text/guide/word_embeddings\n",
    "\n",
    "We map each word into a vector of `embedding_dim` values. Basically, we map the words into an embedding space. The aim of this is to have a vectorial representation of words in which similar words are near to each others.\n",
    "\n",
    "This mapping into the embedding space is learnt. The embedding is put as a layer into our NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "uPwO6VJJ4l6I"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEso15Qw3k4I"
   },
   "source": [
    "## FIRST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "fRuuVxX4XBWh"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Dropout \n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "uz8nagbqXBZK"
   },
   "outputs": [],
   "source": [
    "# Input x: sequence of N word, where each word is an integer/index\n",
    "xin = Input(shape=(N,))\n",
    "\n",
    "# Embedding: we trasform each word in the sequence to a vector of 'embedding_dim' values.\n",
    "# This layer has parameters which must be learnt\n",
    "x = Embedding(n_words, embedding_dim)(xin)\n",
    "\n",
    "h_outputs = LSTM(units=256, return_sequences=True)(x)\n",
    "h_outputs = Dropout(0.2)(h_outputs)\n",
    "\n",
    "# LSTM: it takes in input a sequence of N words, where each word is a vector of 'embedding_dim' values.\n",
    "# We keep only the last output h_N, which is a vector with 128 values\n",
    "last_h = LSTM(units=256)(h_outputs)\n",
    "last_h = Dropout(0.2)(last_h)\n",
    "\n",
    "# Dense layer: it takes in input h_n, and it produces y_hat, which is the categorical distribution over all the possible words, represented as integers\n",
    "y_hat = Dense(units=n_words, activation='softmax')(last_h)\n",
    "\n",
    "model = Model(inputs=xin, outputs=y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pOnoC9PAXBbH",
    "outputId": "b2f71ec9-69d4-4a27-f522-a8bb234f33f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 10)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 10, 128)           649088    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 10, 256)           394240    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 10, 256)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 256)               525312    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5071)              1303247   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,871,887\n",
      "Trainable params: 2,871,887\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3QM8enl5AOZ"
   },
   "source": [
    "As it can be seen, the Embedding layer has many parameters which must be learnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tHTX1oG5FAz"
   },
   "source": [
    "### Compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0g57rqTOF-3Y"
   },
   "source": [
    "We use **sparse categorical crossentropy** as loss function, since our target data `target_integers` contain words represented as integers. The words are represented with the labels, and not with the true categorical distrbution: therefore, we use sparse categorical crossentropy and not simply categorical crossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ZNojKDS7X-uJ"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2-1R--95KM_"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nc_qXKa0YA94",
    "outputId": "1e3a299c-d590-46e4-8d8f-49fca526672d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "232/232 [==============================] - 71s 283ms/step - loss: 6.7901\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 71s 307ms/step - loss: 6.4947\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 78s 337ms/step - loss: 6.3316\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 84s 363ms/step - loss: 6.2270\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 62s 266ms/step - loss: 6.1242\n",
      "Epoch 6/100\n",
      "232/232 [==============================] - 70s 301ms/step - loss: 6.0271\n",
      "Epoch 7/100\n",
      "232/232 [==============================] - 69s 299ms/step - loss: 5.9439\n",
      "Epoch 8/100\n",
      "232/232 [==============================] - 60s 260ms/step - loss: 5.8518\n",
      "Epoch 9/100\n",
      "232/232 [==============================] - 60s 257ms/step - loss: 5.7469\n",
      "Epoch 10/100\n",
      "232/232 [==============================] - 59s 254ms/step - loss: 5.6498\n",
      "Epoch 11/100\n",
      "232/232 [==============================] - 59s 255ms/step - loss: 5.5557\n",
      "Epoch 12/100\n",
      "232/232 [==============================] - 59s 253ms/step - loss: 5.4646\n",
      "Epoch 13/100\n",
      "232/232 [==============================] - 58s 252ms/step - loss: 5.3806\n",
      "Epoch 14/100\n",
      "232/232 [==============================] - 58s 248ms/step - loss: 5.3088\n",
      "Epoch 15/100\n",
      "232/232 [==============================] - 58s 251ms/step - loss: 5.2317\n",
      "Epoch 16/100\n",
      "232/232 [==============================] - 57s 247ms/step - loss: 5.1628\n",
      "Epoch 17/100\n",
      "232/232 [==============================] - 57s 245ms/step - loss: 5.1020\n",
      "Epoch 18/100\n",
      "232/232 [==============================] - 57s 246ms/step - loss: 5.0304\n",
      "Epoch 19/100\n",
      "232/232 [==============================] - 58s 250ms/step - loss: 4.9575\n",
      "Epoch 20/100\n",
      "232/232 [==============================] - 57s 244ms/step - loss: 4.8881\n",
      "Epoch 21/100\n",
      "232/232 [==============================] - 57s 245ms/step - loss: 4.8158\n",
      "Epoch 22/100\n",
      "232/232 [==============================] - 58s 249ms/step - loss: 4.7569\n",
      "Epoch 23/100\n",
      "232/232 [==============================] - 58s 250ms/step - loss: 4.6994\n",
      "Epoch 24/100\n",
      "232/232 [==============================] - 58s 249ms/step - loss: 4.6398\n",
      "Epoch 25/100\n",
      "232/232 [==============================] - 59s 254ms/step - loss: 4.5616\n",
      "Epoch 26/100\n",
      "232/232 [==============================] - 58s 249ms/step - loss: 4.4920\n",
      "Epoch 27/100\n",
      "232/232 [==============================] - 58s 251ms/step - loss: 4.4394\n",
      "Epoch 28/100\n",
      "232/232 [==============================] - 57s 247ms/step - loss: 4.3879\n",
      "Epoch 29/100\n",
      "232/232 [==============================] - 57s 247ms/step - loss: 4.2951\n",
      "Epoch 30/100\n",
      "232/232 [==============================] - 58s 249ms/step - loss: 4.2116\n",
      "Epoch 31/100\n",
      "232/232 [==============================] - 61s 261ms/step - loss: 4.1418\n",
      "Epoch 32/100\n",
      "232/232 [==============================] - 60s 261ms/step - loss: 4.0808\n",
      "Epoch 33/100\n",
      "232/232 [==============================] - 61s 262ms/step - loss: 4.0306\n",
      "Epoch 34/100\n",
      "232/232 [==============================] - 60s 260ms/step - loss: 3.9647\n",
      "Epoch 35/100\n",
      "232/232 [==============================] - 60s 257ms/step - loss: 3.9184\n",
      "Epoch 36/100\n",
      "232/232 [==============================] - 60s 258ms/step - loss: 3.8803\n",
      "Epoch 37/100\n",
      "232/232 [==============================] - 60s 257ms/step - loss: 3.8446\n",
      "Epoch 38/100\n",
      "232/232 [==============================] - 58s 252ms/step - loss: 3.8015\n",
      "Epoch 39/100\n",
      "232/232 [==============================] - 58s 248ms/step - loss: 3.7516\n",
      "Epoch 40/100\n",
      "232/232 [==============================] - 59s 253ms/step - loss: 3.7022\n",
      "Epoch 41/100\n",
      "232/232 [==============================] - 57s 246ms/step - loss: 3.6532\n",
      "Epoch 42/100\n",
      "232/232 [==============================] - 57s 244ms/step - loss: 3.6135\n",
      "Epoch 43/100\n",
      "232/232 [==============================] - 57s 246ms/step - loss: 3.5786\n",
      "Epoch 44/100\n",
      "232/232 [==============================] - 56s 243ms/step - loss: 3.5334\n",
      "Epoch 45/100\n",
      "232/232 [==============================] - 57s 244ms/step - loss: 3.4868\n",
      "Epoch 46/100\n",
      "232/232 [==============================] - 57s 247ms/step - loss: 3.3955\n",
      "Epoch 47/100\n",
      "232/232 [==============================] - 57s 244ms/step - loss: 3.3477\n",
      "Epoch 48/100\n",
      "232/232 [==============================] - 57s 245ms/step - loss: 3.3205\n",
      "Epoch 49/100\n",
      "232/232 [==============================] - 58s 249ms/step - loss: 3.2857\n",
      "Epoch 50/100\n",
      "232/232 [==============================] - 58s 249ms/step - loss: 3.2514\n",
      "Epoch 51/100\n",
      "232/232 [==============================] - 58s 249ms/step - loss: 3.2309\n",
      "Epoch 52/100\n",
      "232/232 [==============================] - 61s 262ms/step - loss: 3.1670\n",
      "Epoch 53/100\n",
      "232/232 [==============================] - 58s 250ms/step - loss: 3.1278\n",
      "Epoch 54/100\n",
      "232/232 [==============================] - 57s 244ms/step - loss: 3.0904\n",
      "Epoch 55/100\n",
      "232/232 [==============================] - 56s 242ms/step - loss: 3.0663\n",
      "Epoch 56/100\n",
      "232/232 [==============================] - 56s 243ms/step - loss: 3.0530\n",
      "Epoch 57/100\n",
      "232/232 [==============================] - 58s 248ms/step - loss: 3.0306\n",
      "Epoch 58/100\n",
      "232/232 [==============================] - 56s 243ms/step - loss: 3.0257\n",
      "Epoch 59/100\n",
      "232/232 [==============================] - 56s 243ms/step - loss: 3.0002\n",
      "Epoch 60/100\n",
      "232/232 [==============================] - 56s 243ms/step - loss: 2.9678\n",
      "Epoch 61/100\n",
      "232/232 [==============================] - 58s 248ms/step - loss: 2.9329\n",
      "Epoch 62/100\n",
      "232/232 [==============================] - 60s 257ms/step - loss: 2.9066\n",
      "Epoch 63/100\n",
      "232/232 [==============================] - 59s 255ms/step - loss: 2.9039\n",
      "Epoch 64/100\n",
      "232/232 [==============================] - 58s 252ms/step - loss: 2.8822\n",
      "Epoch 65/100\n",
      "232/232 [==============================] - 60s 257ms/step - loss: 2.8600\n",
      "Epoch 66/100\n",
      "232/232 [==============================] - 62s 266ms/step - loss: 2.8497\n",
      "Epoch 67/100\n",
      "232/232 [==============================] - 60s 259ms/step - loss: 2.8362\n",
      "Epoch 68/100\n",
      "232/232 [==============================] - 58s 252ms/step - loss: 2.8150\n",
      "Epoch 69/100\n",
      "232/232 [==============================] - 58s 251ms/step - loss: 2.7939\n",
      "Epoch 70/100\n",
      "232/232 [==============================] - 61s 261ms/step - loss: 2.7799\n",
      "Epoch 71/100\n",
      "232/232 [==============================] - 62s 268ms/step - loss: 2.7673\n",
      "Epoch 72/100\n",
      "232/232 [==============================] - 62s 266ms/step - loss: 2.7501\n",
      "Epoch 73/100\n",
      "232/232 [==============================] - 62s 266ms/step - loss: 2.7077\n",
      "Epoch 74/100\n",
      "232/232 [==============================] - 59s 255ms/step - loss: 2.6848\n",
      "Epoch 75/100\n",
      "232/232 [==============================] - 57s 244ms/step - loss: 2.6718\n",
      "Epoch 76/100\n",
      "232/232 [==============================] - 58s 251ms/step - loss: 2.6666\n",
      "Epoch 77/100\n",
      "232/232 [==============================] - 58s 251ms/step - loss: 2.6480\n",
      "Epoch 78/100\n",
      "232/232 [==============================] - 59s 254ms/step - loss: 2.6490\n",
      "Epoch 79/100\n",
      "232/232 [==============================] - 58s 250ms/step - loss: 2.6299\n",
      "Epoch 80/100\n",
      "232/232 [==============================] - 57s 246ms/step - loss: 2.6332\n",
      "Epoch 81/100\n",
      "232/232 [==============================] - 57s 246ms/step - loss: 2.6386\n",
      "Epoch 82/100\n",
      "232/232 [==============================] - 57s 245ms/step - loss: 2.6352\n",
      "Epoch 83/100\n",
      "232/232 [==============================] - 57s 245ms/step - loss: 2.6427\n",
      "Epoch 84/100\n",
      "232/232 [==============================] - 57s 247ms/step - loss: 2.6177\n",
      "Epoch 85/100\n",
      "232/232 [==============================] - 57s 246ms/step - loss: 2.6125\n",
      "Epoch 86/100\n",
      "232/232 [==============================] - 57s 244ms/step - loss: 2.5731\n",
      "Epoch 87/100\n",
      "232/232 [==============================] - 58s 250ms/step - loss: 2.5609\n",
      "Epoch 88/100\n",
      "232/232 [==============================] - 57s 246ms/step - loss: 2.5502\n",
      "Epoch 89/100\n",
      "232/232 [==============================] - 57s 246ms/step - loss: 2.5276\n",
      "Epoch 90/100\n",
      "232/232 [==============================] - 57s 246ms/step - loss: 2.5054\n",
      "Epoch 91/100\n",
      "232/232 [==============================] - 58s 251ms/step - loss: 2.4920\n",
      "Epoch 92/100\n",
      "232/232 [==============================] - 57s 246ms/step - loss: 2.4713\n",
      "Epoch 93/100\n",
      "232/232 [==============================] - 57s 245ms/step - loss: 2.4494\n",
      "Epoch 94/100\n",
      "232/232 [==============================] - 57s 245ms/step - loss: 2.4339\n",
      "Epoch 95/100\n",
      "232/232 [==============================] - 59s 254ms/step - loss: 2.4232\n",
      "Epoch 96/100\n",
      "232/232 [==============================] - 57s 246ms/step - loss: 2.4076\n",
      "Epoch 97/100\n",
      "232/232 [==============================] - 57s 245ms/step - loss: 2.4263\n",
      "Epoch 98/100\n",
      "232/232 [==============================] - 57s 247ms/step - loss: 2.4096\n",
      "Epoch 99/100\n",
      "232/232 [==============================] - 58s 250ms/step - loss: 2.3952\n",
      "Epoch 100/100\n",
      "232/232 [==============================] - 57s 246ms/step - loss: 2.3883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f46c6b65b90>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " model.fit(inputs_integers, targets_integers, batch_size=128, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_YSbipy5N0A"
   },
   "source": [
    "### Generating text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "bQkodd7jYdMk"
   },
   "outputs": [],
   "source": [
    "def generate_text(l, K):\n",
    "  \"\"\" Generated K new charactes after the given string list of words `l`. List of N words. \"\"\"\n",
    "\n",
    "  for i in range(K):\n",
    "    # We transform the list of words 'l' into the list of corresponding integers/indeces.\n",
    "    # Basically, we transform each word w into the corresponding integer/index.\n",
    "    x = [word2index[w] for w in l]\n",
    "    # We add the batch dimension into x, since our NN processes only batches. The shape of x_batch is 1*N\n",
    "    x_batch = np.expand_dims(x, 0)\n",
    "\n",
    "    # We apply the NN, and we get the predicted categorical distribution for the next character. Actually, we get a batch\n",
    "    y_hat_batch = model.predict(x_batch)\n",
    "    # We extract the categorical distribution from the batch\n",
    "    y_hat = y_hat_batch[0]\n",
    "\n",
    "    # Predicted word, corresponding to the word with higher probabiliry\n",
    "    w_hat = index2word[np.argmax(y_hat)]\n",
    "    # We print that\n",
    "    print(w_hat, end=\" \")\n",
    "\n",
    "    # We update our list of N words, by removing the first one, and by appending the new one\n",
    "    l.append(w_hat)\n",
    "    l = l[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MEML_IF1bydw",
    "outputId": "c8cb887a-2464-4f9c-e83e-7ea32036d1e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the gryphon very sitting and and an right on his house who had not another right as he used to "
     ]
    }
   ],
   "source": [
    "l = ['alice', 'said', 'she', 'wanted', 'to', 'go', 'outside', 'the', 'place', 'where']\n",
    "generate_text(l, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VEctiXnFfl-Q",
    "outputId": "c34481ca-8b95-4e5b-ed60-a0e1c5c882ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the way to hear the rabbit say to talk oh "
     ]
    }
   ],
   "source": [
    "l = ['nor', 'did', 'alice', 'think', 'it', 'so', 'very', 'much', 'out', 'of']\n",
    "generate_text(l, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gP1RrnhDgGN8",
    "outputId": "c73dc31b-1c31-4b3e-faf0-0b1353b00c84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the rabbit, of her time, what did something get to "
     ]
    }
   ],
   "source": [
    "l = ['alice', 'saw', 'the', 'rabbit', 'and', 'the', 'mad', 'hatter', 'and', 'thought']\n",
    "generate_text(l, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGloeGra6Gkk"
   },
   "source": [
    "## VECTORIZATION\n",
    "\n",
    "https://keras.io/examples/nlp/pretrained_word_embeddings/\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization\n",
    "\n",
    "We did by hand the process of mapping words into integers. This could have been done automatically by a keras layer: `TextVectorization`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Egeojn-B6Zqd"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "-4xOvNvn6bwr"
   },
   "outputs": [],
   "source": [
    "# We define our vectorizer, such that it finds the words by splitting using the whitespace. It performs also lower-case transformation and strip of the punctuation\n",
    "vectorizer = TextVectorization(max_tokens=n_words, standardize='lower_and_strip_punctuation', split='whitespace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "alohhAbp6-mY"
   },
   "outputs": [],
   "source": [
    "# Compute the voucaboulary on our dataset, which is a list of words\n",
    "vectorizer.adapt(words_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q45jtIhS7Le_",
    "outputId": "b5a4a3f5-c641-4c71-e70a-73fbc0bc32ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29697\n",
      "['project', 'gutenbergs', 'alices', 'adventures', 'in']\n"
     ]
    }
   ],
   "source": [
    "print(len(words_text))\n",
    "print(words_text[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2AGGYnOe61WA",
    "outputId": "b8acb28c-56f2-486f-f650-4f997eabc4f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29697, 1)\n",
      "tf.Tensor(\n",
      "[[  48]\n",
      " [1517]\n",
      " [ 243]\n",
      " [ 370]\n",
      " [  11]], shape=(5, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Compute the integers/indeces corresponding to our words\n",
    "print(vectorizer(words_text).shape)\n",
    "print(vectorizer(words_text)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LCu9xmsQ64Jl",
    "outputId": "cfb52945-11f2-4d52-e4cb-8f96bf879be8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'and', 'to']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the vocabulary. It is a list of words, ordered according to their integer/index\n",
    "voc = vectorizer.get_vocabulary()\n",
    "voc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "6qjnyKmG7-4j"
   },
   "outputs": [],
   "source": [
    "# Build the dictionary for the mapping word -> index\n",
    "word2index = dict(zip(voc, range(len(voc))))\n",
    "\n",
    "# Build the dictionary for the mapping index -> word\n",
    "index2word = dict(zip(range(len(voc)), voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "btkWa-LL8nfj",
    "outputId": "4be52f74-66b2-465f-c959-a90e75acc972"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5071\n",
      "3255\n"
     ]
    }
   ],
   "source": [
    "# The size of our new vocubulary is smaller than the number of all possible words computed before. Because now we have removed the punctuation\n",
    "print(n_words)\n",
    "print(len(voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "qBjYfLKF8eHP",
    "outputId": "f4c814da-7a6d-44f3-e55a-b0dbb1069daa"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-87b624b0a0ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mindex2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alice.'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'alice.'"
     ]
    }
   ],
   "source": [
    "index2word[word2index['alice.']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gaj4W79m8484",
    "outputId": "b3d66d91-5f34-4076-a6ec-0fb75227e823"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[13]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[13]], shape=(1, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer(['alice.']))\n",
    "print(vectorizer(['alice']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6EbfWMkHIt3"
   },
   "source": [
    "This is the actual size of our new vocubulary. This is the actual number of possible different words, instead of the old `n_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "nX7XMIs6HCc4"
   },
   "outputs": [],
   "source": [
    "n_words = len(voc)+2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFQostufB9AH"
   },
   "source": [
    "### Transform the dataset using the vectorizer\n",
    "We transform `inputs` and `targets`, which contain words represented as strings, to `inputs_integers` and `targets_integers`, which contain words represented as integers/indeces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "akaaHuN7CA1L"
   },
   "outputs": [],
   "source": [
    "inputs_integers = vectorizer(np.array([s for s in inputs]).reshape((M,N,1))).numpy().reshape((M,N))\n",
    "targets_integers = vectorizer(np.array([s for s in targets])).numpy().reshape((M,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L6-MJeKPCzpj",
    "outputId": "9eb9630b-56f2-4207-ddd1-6ad2c17d6eec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29687, 10)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_integers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ydbynRvYE09e",
    "outputId": "0701950b-6d60-4816-a031-dee27ba1859f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  48, 1517,  243,  370,   11,  448,   60,  848,  913,   22])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_integers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yBzWxQ-LE0pq",
    "outputId": "0afee972-b37a-42f5-b6ef-86791e9d993f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29687,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_integers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GBILEC0-E_u3",
    "outputId": "457934b2-82a3-4e25-8738-b7b6613faec5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "437"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_integers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSbOlkEr9QZ3"
   },
   "source": [
    "## PRE-TRAINED EMBEDDING\n",
    "Before we have trained our embedding. We can also import and use a pre-trained embedding. We use the GloVe pre-trained embedding. We use $100$-dimensional embeddings (i.e. $100$ values).\n",
    "\n",
    "https://keras.io/examples/nlp/pretrained_word_embeddings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "SwRwPKnKHdxG"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KGBZrVNa9bbg",
    "outputId": "8433132f-0656-472f-8b76-0c15147c98dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-07-02 12:54:51--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2022-07-02 12:54:51--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2022-07-02 12:54:51--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 39s  \n",
      "\n",
      "2022-07-02 12:57:30 (5.19 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "jdnolDCB9sw0"
   },
   "outputs": [],
   "source": [
    "path_to_glove_file = 'glove.6B.100d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WB_UfA3o9m4k"
   },
   "source": [
    "We load the embedding. We load the mapping from words to embeddings. Map from real words to vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HMGjp6-v9nOV",
    "outputId": "d0eb18de-10b2-4e47-beb2-1d01221db11c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "word2embedding = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, embedding = line.split(maxsplit=1)\n",
    "        embedding = np.fromstring(embedding, \"f\", sep=\" \")\n",
    "        word2embedding[word] = embedding\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(word2embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VS-XVePh-Htj"
   },
   "source": [
    "Now we build the mapping from our integers/indeces to embeddings. With our integers/indeces we mean the ones built before using the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UYQOphTs-H0b",
    "outputId": "d4a2d1d1-151f-40dc-deec-67b8d577d4b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 2970 words (285 misses)\n"
     ]
    }
   ],
   "source": [
    "# We count the number of words in our (vectorized) dataset which are not found in the imported embedding\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "index2embedding = {}\n",
    "for word, i in word2index.items():\n",
    "    embedding_vector = word2embedding.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in word2embedding will be all-zeros.\n",
    "        word2embedding[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2IDeSDrs_DeY"
   },
   "source": [
    "Now, for using our pre-trained embedding, we feed it into an `Embedding` layer. Basically, we give to the `Embedding` layer the `index2embedding` dictionary. In this way, the pre-trained embedding will trasform words represented as integers/indeces to words represented as embedding.\n",
    "\n",
    "Actually, the `Embedding` layer does not accept a `dict` datatype: he want a numpy array. Let's transform the `index2embedding` dictionary into a numpy array: numpy matrix which containes, in the row $i$, the embedding for that integer/index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "BtfOaUAv_C-w"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((n_words, embedding_dim))\n",
    "for i, embedding_vector in index2embedding.items():\n",
    "    embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGo1kf8CAeGc"
   },
   "source": [
    "## MODEL\n",
    "Let's now define again our model. We use the vectorizer and the pre-trained Embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "7JIjzNZ4BQZu"
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras as ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "Ylk6vAQFAzsa"
   },
   "outputs": [],
   "source": [
    "# Input x: sequence of N word, where each word is an index/integer\n",
    "xin = Input(shape=(N,))\n",
    "\n",
    "# Embedding: we put as embedding layer our pre-trained embedding.\n",
    "# It transforms a word represented as an index to an embedding vector, with 'embedding_dim' values\n",
    "embedding_layer = Embedding(\n",
    "    n_words,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=ks.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,  # WE SET IT AS NON-TRAINABLE\n",
    ")\n",
    "x = embedding_layer(xin)\n",
    "\n",
    "h_outputs = LSTM(units=256,  return_sequences=True)(x)\n",
    "h_outputs = Dropout(0.2)(h_outputs)\n",
    "\n",
    "# LSTM: it takes in input a sequence of N words, where each word is a vector of 'embedding_dim' values.\n",
    "# We keep only the last output h_N, which is a vector with 128 values\n",
    "last_h = LSTM(units=256)(h_outputs)\n",
    "last_h = Dropout(0.2)(last_h)\n",
    "\n",
    "# Dense layer: it takes in input h_n, and it produces y_hat, which is the categorical distribution over all the possible words, represented as integers\n",
    "y_hat = Dense(units=n_words)(last_h) #activation='softmax')(last_h)\n",
    "\n",
    "model = Model(inputs=xin, outputs=y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_4wR0UnnAzsb",
    "outputId": "59fc83ef-8236-4b57-c086-cd2b86c725f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 10)]              0         \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, 10, 100)           325700    \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 10, 256)           365568    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 10, 256)           0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 256)               525312    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3257)              837049    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,053,629\n",
      "Trainable params: 1,727,929\n",
      "Non-trainable params: 325,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KLK_5XkAzsd"
   },
   "source": [
    "As it can be seen, the Embedding layer has all the parameters which are non-trainable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNcVcBc7Azsd"
   },
   "source": [
    "### Compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuCw2wY5FNvn"
   },
   "source": [
    "We use **sparse categorical crossentropy** as loss function, since our target data `target_integers` contain words represented as integers. The words are represented with the labels, and not with the true categorical distrbution: therefore, we use sparse categorical crossentropy and not simply categorical crossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "mkPnjc0qIoth"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam \n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "9nvsAo4UAzsd"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=SparseCategoricalCrossentropy(from_logits=True), optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4SmoFeoGIpy"
   },
   "outputs": [],
   "source": [
    "# DELETE\n",
    "targets_oneHotEncoded = np.zeros((M,n_words))\n",
    "for i,word_idx in enumerate(targets_integers):\n",
    "  targets_oneHotEncoded[i, word_idx] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_kcMCgVGnsL"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgMLXpE7Azsd"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KqIRpLHiAzsd",
    "outputId": "1f987006-cae0-4b6a-ba83-b8d5e81ffbb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "232/232 [==============================] - 53s 213ms/step - loss: 6.4577\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 50s 218ms/step - loss: 6.2190\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 49s 211ms/step - loss: 6.2083\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 49s 211ms/step - loss: 6.2016\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 49s 210ms/step - loss: 6.2004\n",
      "Epoch 6/100\n",
      "232/232 [==============================] - 49s 211ms/step - loss: 6.1997\n",
      "Epoch 7/100\n",
      "232/232 [==============================] - 49s 211ms/step - loss: 6.1987\n",
      "Epoch 8/100\n",
      "232/232 [==============================] - 52s 222ms/step - loss: 6.1973\n",
      "Epoch 9/100\n",
      "232/232 [==============================] - 49s 211ms/step - loss: 6.1956\n",
      "Epoch 10/100\n",
      "232/232 [==============================] - 49s 210ms/step - loss: 6.1947\n",
      "Epoch 11/100\n",
      "232/232 [==============================] - 49s 211ms/step - loss: 6.1946\n",
      "Epoch 12/100\n",
      "232/232 [==============================] - 49s 211ms/step - loss: 6.1937\n",
      "Epoch 13/100\n",
      "232/232 [==============================] - 49s 212ms/step - loss: 6.1918\n",
      "Epoch 14/100\n",
      "232/232 [==============================] - 49s 213ms/step - loss: 6.1920\n",
      "Epoch 15/100\n",
      "232/232 [==============================] - 50s 216ms/step - loss: 6.1922\n",
      "Epoch 16/100\n",
      "232/232 [==============================] - 49s 211ms/step - loss: 6.1919\n",
      "Epoch 17/100\n",
      "232/232 [==============================] - 49s 211ms/step - loss: 6.1903\n",
      "Epoch 18/100\n",
      "232/232 [==============================] - 49s 213ms/step - loss: 6.1898\n",
      "Epoch 19/100\n",
      "232/232 [==============================] - 49s 213ms/step - loss: 6.1890\n",
      "Epoch 20/100\n",
      "232/232 [==============================] - 51s 218ms/step - loss: 6.1883\n",
      "Epoch 21/100\n",
      "232/232 [==============================] - 52s 223ms/step - loss: 6.1888\n",
      "Epoch 22/100\n",
      "232/232 [==============================] - 50s 216ms/step - loss: 6.1860\n",
      "Epoch 23/100\n",
      "232/232 [==============================] - 49s 211ms/step - loss: 6.1867\n",
      "Epoch 24/100\n",
      "232/232 [==============================] - 49s 212ms/step - loss: 6.1866\n",
      "Epoch 25/100\n",
      "232/232 [==============================] - 50s 215ms/step - loss: 6.1854\n",
      "Epoch 26/100\n",
      "232/232 [==============================] - 51s 219ms/step - loss: 6.1850\n",
      "Epoch 27/100\n",
      "232/232 [==============================] - 50s 214ms/step - loss: 6.1844\n",
      "Epoch 28/100\n",
      "232/232 [==============================] - 52s 222ms/step - loss: 6.1844\n",
      "Epoch 29/100\n",
      "232/232 [==============================] - 52s 222ms/step - loss: 6.1837\n",
      "Epoch 30/100\n",
      "232/232 [==============================] - 52s 224ms/step - loss: 6.1849\n",
      "Epoch 31/100\n",
      "232/232 [==============================] - 52s 224ms/step - loss: 6.1837\n",
      "Epoch 32/100\n",
      "232/232 [==============================] - 52s 224ms/step - loss: 6.1819\n",
      "Epoch 33/100\n",
      "232/232 [==============================] - 51s 218ms/step - loss: 6.1829\n",
      "Epoch 34/100\n",
      "232/232 [==============================] - 50s 215ms/step - loss: 6.1825\n",
      "Epoch 35/100\n",
      "232/232 [==============================] - 51s 220ms/step - loss: 6.1826\n",
      "Epoch 36/100\n",
      "232/232 [==============================] - 50s 215ms/step - loss: 6.1826\n",
      "Epoch 37/100\n",
      "232/232 [==============================] - 50s 217ms/step - loss: 6.1816\n",
      "Epoch 38/100\n",
      "232/232 [==============================] - 53s 227ms/step - loss: 6.1816\n",
      "Epoch 39/100\n",
      "232/232 [==============================] - 53s 227ms/step - loss: 6.1808\n",
      "Epoch 40/100\n",
      "232/232 [==============================] - 53s 226ms/step - loss: 6.1812\n",
      "Epoch 41/100\n",
      "232/232 [==============================] - 53s 227ms/step - loss: 6.1797\n",
      "Epoch 42/100\n",
      "232/232 [==============================] - 51s 222ms/step - loss: 6.1800\n",
      "Epoch 43/100\n",
      "232/232 [==============================] - 49s 212ms/step - loss: 6.1809\n",
      "Epoch 44/100\n",
      "232/232 [==============================] - 51s 219ms/step - loss: 6.1801\n",
      "Epoch 45/100\n",
      "232/232 [==============================] - 51s 218ms/step - loss: 6.1799\n",
      "Epoch 46/100\n",
      "232/232 [==============================] - 51s 220ms/step - loss: 6.1798\n",
      "Epoch 47/100\n",
      "232/232 [==============================] - 51s 220ms/step - loss: 6.1794\n",
      "Epoch 48/100\n",
      "232/232 [==============================] - 52s 223ms/step - loss: 6.1787\n",
      "Epoch 49/100\n",
      "232/232 [==============================] - 52s 225ms/step - loss: 6.1778\n",
      "Epoch 50/100\n",
      "232/232 [==============================] - 51s 220ms/step - loss: 6.1782\n",
      "Epoch 51/100\n",
      "232/232 [==============================] - 52s 225ms/step - loss: 6.1781\n",
      "Epoch 52/100\n",
      "232/232 [==============================] - 49s 213ms/step - loss: 6.1773\n",
      "Epoch 53/100\n",
      "232/232 [==============================] - 49s 212ms/step - loss: 6.1776\n",
      "Epoch 54/100\n",
      "232/232 [==============================] - 49s 212ms/step - loss: 6.1777\n",
      "Epoch 55/100\n",
      "232/232 [==============================] - 50s 216ms/step - loss: 6.1781\n",
      "Epoch 56/100\n",
      "232/232 [==============================] - 49s 212ms/step - loss: 6.1770\n",
      "Epoch 57/100\n",
      "232/232 [==============================] - 50s 217ms/step - loss: 6.1776\n",
      "Epoch 58/100\n",
      "232/232 [==============================] - 52s 222ms/step - loss: 6.1762\n",
      "Epoch 59/100\n",
      "232/232 [==============================] - 52s 224ms/step - loss: 6.1760\n",
      "Epoch 60/100\n",
      "232/232 [==============================] - 51s 221ms/step - loss: 6.1769\n",
      "Epoch 61/100\n",
      "232/232 [==============================] - 50s 216ms/step - loss: 6.1767\n",
      "Epoch 62/100\n",
      "232/232 [==============================] - 52s 225ms/step - loss: 6.1762\n",
      "Epoch 63/100\n",
      "232/232 [==============================] - 50s 215ms/step - loss: 6.1754\n",
      "Epoch 64/100\n",
      "232/232 [==============================] - 49s 213ms/step - loss: 6.1760\n",
      "Epoch 65/100\n",
      "232/232 [==============================] - 50s 214ms/step - loss: 6.1750\n",
      "Epoch 66/100\n",
      "232/232 [==============================] - 50s 215ms/step - loss: 6.1752\n",
      "Epoch 67/100\n",
      "232/232 [==============================] - 51s 219ms/step - loss: 6.1761\n",
      "Epoch 68/100\n",
      "232/232 [==============================] - 50s 216ms/step - loss: 6.1743\n",
      "Epoch 69/100\n",
      "232/232 [==============================] - 53s 227ms/step - loss: 6.1747\n",
      "Epoch 70/100\n",
      "232/232 [==============================] - 51s 219ms/step - loss: 6.1746\n",
      "Epoch 71/100\n",
      "232/232 [==============================] - 51s 218ms/step - loss: 6.1745\n",
      "Epoch 72/100\n",
      "232/232 [==============================] - 51s 220ms/step - loss: 6.1743\n",
      "Epoch 73/100\n",
      "232/232 [==============================] - 51s 219ms/step - loss: 6.1748\n",
      "Epoch 74/100\n",
      "232/232 [==============================] - 52s 225ms/step - loss: 6.1747\n",
      "Epoch 75/100\n",
      "232/232 [==============================] - 51s 222ms/step - loss: 6.1736\n",
      "Epoch 76/100\n",
      "232/232 [==============================] - 50s 217ms/step - loss: 6.1733\n",
      "Epoch 77/100\n",
      "232/232 [==============================] - 49s 212ms/step - loss: 6.1729\n",
      "Epoch 78/100\n",
      "232/232 [==============================] - 49s 212ms/step - loss: 6.1742\n",
      "Epoch 79/100\n",
      "232/232 [==============================] - 49s 212ms/step - loss: 6.1730\n",
      "Epoch 80/100\n",
      "232/232 [==============================] - 49s 213ms/step - loss: 6.1724\n",
      "Epoch 81/100\n",
      "232/232 [==============================] - 49s 213ms/step - loss: 6.1725\n",
      "Epoch 82/100\n",
      "232/232 [==============================] - 50s 214ms/step - loss: 6.1725\n",
      "Epoch 83/100\n",
      "232/232 [==============================] - 50s 217ms/step - loss: 6.1734\n",
      "Epoch 84/100\n",
      "232/232 [==============================] - 50s 213ms/step - loss: 6.1726\n",
      "Epoch 85/100\n",
      "232/232 [==============================] - 49s 213ms/step - loss: 6.1730\n",
      "Epoch 86/100\n",
      "232/232 [==============================] - 49s 213ms/step - loss: 6.1728\n",
      "Epoch 87/100\n",
      "232/232 [==============================] - 49s 213ms/step - loss: 6.1738\n",
      "Epoch 88/100\n",
      "232/232 [==============================] - 49s 212ms/step - loss: 6.1727\n",
      "Epoch 89/100\n",
      "232/232 [==============================] - 49s 213ms/step - loss: 6.1717\n",
      "Epoch 90/100\n",
      "232/232 [==============================] - 50s 217ms/step - loss: 6.1722\n",
      "Epoch 91/100\n",
      "232/232 [==============================] - 50s 215ms/step - loss: 6.1718\n",
      "Epoch 92/100\n",
      "232/232 [==============================] - 49s 213ms/step - loss: 6.1718\n",
      "Epoch 93/100\n",
      "232/232 [==============================] - 49s 213ms/step - loss: 6.1713\n",
      "Epoch 94/100\n",
      "232/232 [==============================] - 50s 214ms/step - loss: 6.1712\n",
      "Epoch 95/100\n",
      "232/232 [==============================] - 49s 213ms/step - loss: 6.1709\n",
      "Epoch 96/100\n",
      "232/232 [==============================] - 50s 214ms/step - loss: 6.1715\n",
      "Epoch 97/100\n",
      "232/232 [==============================] - 51s 219ms/step - loss: 6.1713\n",
      "Epoch 98/100\n",
      "232/232 [==============================] - 51s 218ms/step - loss: 6.1711\n",
      "Epoch 99/100\n",
      "232/232 [==============================] - 50s 216ms/step - loss: 6.1701\n",
      "Epoch 100/100\n",
      "232/232 [==============================] - 52s 222ms/step - loss: 6.1711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f46bd1a4e50>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " model.fit(inputs_integers, targets_integers, batch_size=128, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOW-ks0RAzsd"
   },
   "source": [
    "### Generating text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CuDSu-0Azsd",
    "outputId": "fdcefb2f-94c4-4b94-c905-ad3e8a01bb0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the the the the the the the the the the the the the the the the the the the the "
     ]
    }
   ],
   "source": [
    "l = ['alice', 'said', 'she', 'wanted', 'to', 'go', 'outside', 'the', 'place', 'where']\n",
    "generate_text(l, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IeCaBCr_Azsd",
    "outputId": "ab75e8ef-ae3a-481f-c11e-8739ef2c168d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the the the the the the the the the the "
     ]
    }
   ],
   "source": [
    "l = ['nor', 'did', 'alice', 'think', 'it', 'so', 'very', 'much', 'out', 'of']\n",
    "generate_text(l, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sv1by-YBAzsd",
    "outputId": "af8d1e02-242d-41d6-a678-81151edd7de1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the the the the the the the the the the "
     ]
    }
   ],
   "source": [
    "l = ['alice', 'saw', 'the', 'rabbit', 'and', 'the', 'mad', 'hatter', 'and', 'thought']\n",
    "generate_text(l, 10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "5 Example Word-based LanguageModeling",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
